So it is generally accepted that top tier programmers can produce an order of magnitude more/better code than their more average peers. It's also generally accepted that the rate of errors made in code is **relatively constant** for programmers. Instead, it tends to be impacted by the processes used when writing the code and after the code is written. (As I understand it) Humans tend to make mistakes at a fairly constant rate - better programmers just notice more of them and are quicker to fix them.   * Note that both of above assertions come from Code Complete by Steve McConnell - so it's not a matter of differing perspectives. So I've started to see this recently in my code. I can hammer out about 4-5x the amount of code as many of my peers (measured by story points estimated by the team), with higher quality (based on performance metrics and number of changes made after check-in). But I still make mistakes. Between better unit tests, a better understanding of what the code is doing, and a better eye for issues when doing code reviews I'm not producing 4-5x the number of bugs. But I'm still producing about _twice_ as many bugs found by QA as other developers on my team. As you might imagine, this causes some problems with non-technical folks doing metric measurements (read: my boss). I've tried to point out that I'm producing bugs at half the rate of my peers (and fix twice as many), but it's a hard sell when there's graphs saying I produce twice as many bugs. So, how to deal with the fact that increased productivity will lead to an increased number of bugs?