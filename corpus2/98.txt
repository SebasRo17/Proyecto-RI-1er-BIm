I often need to compute the eigenvalues of large matrices, and I invariably resort to MATLAB for these, simply because it is much faster. I'd like to change that, so that I can work entirely inside my notebook. Here's a plot comparing the timings between the two for eigenvalue decompositions for matrices of varying sizes (left). The y-axis shows the time in seconds. As you can see, there's about a factor 3 difference between the two (right). ![enter image description here](http://i.stack.imgur.com/w8cAnm.jpg) ![enter image description here](http://i.stack.imgur.com/3i8V5m.jpg) Here's a sample code in Mathematica to generate timings:               timings = With[{x = RandomReal[NormalDistribution[], {#, #}]},        Eigenvalues[x]; // Timing // First] & /@ Range[500,5000,500]      and its equivalent in MATLAB:               s = 500:500:5000;     t = zeros(numel(s),1);     for i = 1:numel(s)         x=randn(s(i));         t1=tic;eig(x);t(i)=toc(t1);     end      I do not think that Mathematica's algorithms are inefficient, as the fastest algorithms for eigenvalue decompositions (in the general case, not exploiting symmetry and such) are $\mathcal{O}(N^{2.376})$ and the timings both MATLAB's and Mathematica's implementations have the same correct slope on a log-log plot. I suspected unpacking in the background during the call to `Eigenvalues` and turning `On["Packing"]` confirms this. However, I don't think this alone could be the cause for a 3 fold speed reduction. I'm not expecting the timings to be exact either, as I understand that arrays and matrices are baked into the core of one and not the other, which can lead to performance differences. However, I'm interested in knowing if   1. there are reasons other than the simplified one I gave above for the difference in timings and    2. there ways in which I can improve the speeds or at least, reduce the difference by _some_ amount. Or is this something that one has to accept as a fact of life?