I believe I'm obtaining overflow errors when randomly sampling from a log- normal distribution with the command:               RandomVariate[LogNormalDistribution[μ, σ], 1]      Specifically, I can obtain an accurate looking distribution with values of $\mu \approx 1$ and $\sigma \approx 1$, but I get significant outliers for values of $\mu > 10$ and $\sigma > 10$. Why would this occur? Isn't it true that the ratio $\dfrac{\mu}{\sigma}$ should govern the probability of obtaining values $\gg \mu$? What values of $\mu$ and $\sigma$ will give accurate values properly reflecting a random sample from `LogNormalDistribution`?