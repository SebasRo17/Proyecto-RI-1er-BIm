I'm a planetary science researcher and one project I'm working on is _N_ -body simulations of Saturn's rings. The goal of this particular study is to watch as particles clump together under their own self-gravity and measure the aggregate mass of the clumps versus the mean velocity of all particles in the cell. We're trying to figure out if this can explain some observations made by the _Cassini_ spacecraft during the Saturnian summer solstice when large structures were seen casting shadows on the nearly edge-on rings. Below is a screenshot of what any given timestep looks like. (Each particle is 2 m in diameter and the simulation cell itself is around 700 m across.) ![_N_-body cell of a simulation of Saturn's rings with particles shown as tiny shaded spheres against a black background.](http://i.stack.imgur.com/XUsR7.png) The code I'm using already spits out the mean velocity at every timestep. What I need to do is figure out a way to determine the mass of particles in the clumps and NOT the stray particles between them. I know every particle's position, mass, size, etc., but I don't know easily that, say, particles 30,000-40,000 along with 102,000-105,000 make up one strand that to the human eye is obvious. So, the algorithm I need to write would need to be a code with as few user- entered parameters as possible (for replicability and objectivity) that would go through all the particle positions, figure out what particles belong to clumps, and then calculate the mass. It would be great if it could do it for "each" clump/strand as opposed to everything over the cell, but I don't think I actually _need_ it to separate them out. The only thing I was thinking of was doing some sort of _N_ 2 distance calculation where I'd calculate the distance between every particle and if, say, the closest 100 particles were within a certain distance, then that particle would be considered part of a cluster. But that seems pretty sloppy and I was hoping that you CS folks and programmers might know of a more elegant solution? * * * **Edited with My Solution:** What I did was to take a sort of nearest-neighbor / cluster approach and do the quick-n-dirty _N_ 2 implementation first. So, take every particle, calculate distance to all other particles, and the threshold for in a cluster or not was whether there were _N_ particles within _d_ distance (two parameters that have to be set _a priori_ , unfortunately, but as was said by some responses/comments, I wasn't going to get away with not having some of those). I then sped it up by not sorting distances but simply doing an order _N_ search and increment a counter for the particles within _d_ , and that sped stuff up by a factor of 6. Then I added a "stupid programmer's tree" (because I know next to nothing about tree codes). I divide up the simulation cell into a set number of grids (best results when grid size ≈7 _d_ ) where the main grid lines up with the cell, one grid is offset by half in _x_ and _y_ , and the other two are offset by 1/4 in ± _x_ and ± _y_. The code then divides particles into the grids, then each particle _N_ only has to have distances calculated to the other particles in that cell. Theoretically, if this were a real tree, I should get order _N_ *log( _N_ ) as opposed to _N_ 2 speeds. I got somewhere between the two, where for a 50,000-particle sub-set I got a 17x increase in speed, and for a 150,000-particle cell, I got a 38x increase in speed. 12 seconds for the first, 53 seconds for the second, 460 seconds for a 500,000-particle cell. Those are comparable speeds to how long the code takes to run the simulation 1 timestep forward, so that's reasonable at this point. Oh -- and it's fully threaded, so it'll take as many processors as I can throw at it.