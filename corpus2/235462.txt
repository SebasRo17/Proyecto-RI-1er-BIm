We have a stream of data we're aggregating stats over and putting into a python dictionary (sent from kafka). Every X time interval, We update our database with these aggregated metrics. This process works fine, but is not completely fail-safe. What if the python process dies and we lose that time interval's data, for example?   What are recommended methods/tools/etc.. to aggregate streaming data in memory while still having a fallback if the process dies mid time interval?