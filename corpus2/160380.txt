I am building an application that will be modular, in a way that it will be a set of separate systems communicating with each other. It uses Hadoop on all systems, and HBase on 3 of the 4. Scaling will only be an issue on the non Hbase system, it uses it's own NoSQL like system. It will be a client facing system, and is very memory intensive, so each server can handle a limited number of users, before drastically dropping performance. All 4 systems have Zookeeper procedures & options built in. Is it smart to design a command and control server, that will monitor system resources and spin up/spin down servers at times of peak? Would that set me up for failure, since if it faults the whole system can fail? How hard would it be to automate a task like that for Hadoop?