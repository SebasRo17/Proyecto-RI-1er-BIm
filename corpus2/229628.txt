I have to write an HTTP API that wraps any Scrapy spider, it should accept Requests, execute them in Scrapy, and return data extracted by the spider and lets the user reuse the same code to extract interactively. Scrapy supports crawling in batch mode very well, i.e. a long running process starting from seed requests, extracting and following links and exporting data. How should I go about doing this ? What are the technologies involved ? Where can I learn those ? Any help would be greatly appreciated.