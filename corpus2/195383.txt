I've been working on improving an existing library to be more maintainable after trying to implement a feature I wanted and found the code to be hard to alter. The library is a node.js grunt plugin called grunt-contrib-jshint and my implementation is in a repo called grunt-jshint-bfs. I've run a code analysis tool called plato that generates a source analysis report that includes metrics line total/average lines of code, maintainability and estimated errors in an effort to compare the code bases for code quality purposes. The trouble I'm having is comparing the estimated errors between the two projects, since one has 2 files and the other has many more files it would seem that the exposure to errors would increase, or am I reading that wrong? Here is a link to my repo's report and the grunt-contrib-jshint report for comparison.