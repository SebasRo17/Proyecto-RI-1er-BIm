I'm talking about knowing an area of computer science or a language, and then forgetting it over the years. As I approach the 20 year mark, I'm finding myself realizing that I've forgotten more than I currently know. Is this bad? Is this good? Is there a way to solve this? And if not, which things are worth memorizing? For instance, we all know (I think) that Quicksort provides O(n*log n) in average cases, and we know it can balloon to O(N^2) when fed with the wrong inputs. How many of us, if asked without notice, could write a functioning Quicksort on the whiteboard or in their code editor of choice, without blinking? Is this useful? And if we can do it, does it count if it's a naive implementation, and not the optimized version that was engineered by Bentley & Company? I give that as an example, but it could be anything. C++ has many dark corners, that if not used for some length of time, can be easily forgotten. And let's face it, there are many dark corners in C++ that one prefers to steer clear of.