Here's a *hypothetical* issue. There is a performance issue in the current release. Its difficult to find the cause of this issue because the performance tests haven't been run in roughly 10+ releases (with anywhere from 10 - 100 changes per release). My idea is to automate the process of running the performance tests in each release. However, this automation took a bit longer than expected to get running. My (again *hypothetical*) response to the automation taking too long would be to simply perform the tests manually, and then run through the results with a set of scripts that I'd already written. In the end, this automation would be SOO beneficial to have, but it _could_ take longer than is normal for a task to be completed. Is that the right response to what could feel like a rapidly diminishing ROI on a proposed solution?