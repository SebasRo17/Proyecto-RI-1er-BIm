In many languages, the substring function works like this: substring(startIndex, endIndex) returns the substring from startIndex until endIndex-1 (if you view startIndex and endIndex as 0-based) / from startIndex+1 to endIndex (1-based) This is confusing. I understand that the two parameters can be interpreted as "startIndex" and "length of the substring", but in my opinion that is still confusing and even in this case, startIndex is 0-based while length is 1-based. Why not stick to one convention for both the function arguments? and why do newer languages like ruby and python continue to stick to this standard?