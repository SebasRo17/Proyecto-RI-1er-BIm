Edit: Comments suggested, that DSM just faded out by being not used recently. What were the reasons for this, what are DSMs drawbacks? Literature lists many positive aspects like easy to port programms to, no marshalling and therefore faster and so on but also negative aspects like only usable in homogenous environments due to endianess and word size issues. So why is all data synchronizing done by databases and not DSM anymore? Has there been a historic comparison or study at the time when both ways existed concurrently? Old text: Ages ago, Djikstra invented IPC through mutexes which then somehow led to shared memory (SHM) in multics (which afaik had the necessary mmap first). Then computer networks came up and DSM (distributed SHM) was invented for IPC between computers. So DSM is basically a not prestructured memory region (like a SHM) that _magically_ get's synchronized between computers without the applications programmer taking action. Implementations include Treadmarks (inofficially dead now) and CRL. But then someone thought this is not the right way to do it and invented Linda & tuplespaces. Current implementations include JavaSpaces and GigaSpaces. Here, you have to structure your data into tuples. Other ways to achieve similar effects may be the use of a relational database or a key-value-store like RIAK. Although someone might argue, I don't consider them as DSM since there is no coherent memory region where you can put data structures in as you like but have to structure your data which can be hard if it is continuous and administration like locking can not be done for hard coded parts (=tuples, ...). Why is there no DSM implementation today or am I just unable to find one?