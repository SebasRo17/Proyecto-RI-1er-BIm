**The focus of this question:** Some software performs "extra work" in order to increase the chance of a "eventually successful/satisfactory" outcome, despite one or more internal errors in the software, which requires a longer execution time when those errors happen. All these happen without the user's knowledge if the outcome was successful. **Definition of complex software:**   * Contains code written by (contributed from) more than 10 developers over its lifetime, and not written in the same time frame   * Depends on more than 10 external libraries, each with caveats   * A typical software task (for generating a result wanted by the user) requires 10 or more input parameters, where most of them have default values but are configurable if the user needs control.   * Most importantly, software that has the appropriate complexity relative to the task being performed, i.e. _not unnecessarily complicated_. _Edited:_ What is complex? Please see There is a big difference between Complex and Complicated. (direct link) **Definition of Redundancy/Robustness** _within this question_ :    _(Added Robustness based on comments)_   * If a software task failed when the current set of parameters was used, try different parameters.      * Obviously, there must be inside knowledge that those "different" parameters use a different code path, possibly resulting in a different (hopefully better) outcome.     * Sometimes these different code path are chosen based on observations of the external libraries.   * At the end, if the actual task performed is slightly different from the user's specification, the user will receive a report detailing the discrepancy.   * Finally, like the 10-plus configurable parameters, the redundancy and reporting are also configurable. Example of such software:   * Database Migration      * Business database     * Source control database, etc.   * Batch converting between a Word document and an OpenOffice document, PowerPoint and OpenOffice Draw, etc.   * Automatic translation of an entire website   * Automatic analysis of software package, such as Doxygen, but where the analysis needs to be more dependable (i.e. not just a documentation tool)   * Network communication, where packets may be lost and a number of retries are expected _This question was originally inspired fromHow do you deal with intentionally bad code?   but is now focused on just one of the causes of software bloat. This question does not address any other causes of software bloat, such as addition of new features._ _Possibly related:_   * _How to deal with complex codes in (huge) projects_   * _How do people manage to write and maintain extremely complex and hard to read code?_