Working on some large systems using `DiscreteMarkovProcess`, I changed the transition matrix to machine precision vs using exact values, which sped things up handily. The only problem was for edge cases, accuracy suffered. I expected that. However, when probing the limits using simple cases, I was surprised to find this happens when I'd not expected it. A simplified test example:               Table[      intleg = tleg/2;           sa = SparseArray[{{i_, i_} :>           1 - (intleg - i + 1)/tleg, {i_, j_} /;            j == i + 1 :> (1 - i + intleg)/tleg}, {intleg + 1, intleg + 1}];                saN = N[sa, 50];           mp = DiscreteMarkovProcess[1, sa];      mpN = DiscreteMarkovProcess[1, saN];           {tleg, N@Mean[FirstPassageTimeDistribution[mp, intleg + 1]],       N@Mean[FirstPassageTimeDistribution[mpN, intleg + 1]]},           {tleg, {10, 20, 30, 40}}]          (* {{10,22.8333,22.8333},{20,58.5794,58.5793},{30,99.5469,-1.24192},{40,143.91,0.}} *)      This simulates a simple "coupon-collector" situation with a population of `tleg` distinct items, where only half are "interesting" to me. So a transition matrix is built with appropriate probabilities with an absorbing state when all "interesting" are obtained. That same matrix is converted to inexact via `N`. Comparing the results, we see the last two are not even in the zip-code of the correct result. Upping the accuracy/precision of the conversion via `N` seems to have no effect. Any ideas what's going on here (I suspect some kind of catastrophic cancellation because of the characteristics of the probabilities)?