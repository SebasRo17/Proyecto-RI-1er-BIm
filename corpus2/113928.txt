Say I produce some software with a genetic algorithm. In order to be certain that the software works correctly in all situations, I would need to test all situations, which could be prohibitively (or infinitely) many. So I evolve the software with evolving test input (some fixed, some inherited, and some random). Now, with each generation, the probability measurably increases that the fittest functions will be general solutions, not just solutions for the known data at hand. As developers, we are flawed, and our software may fail, in fact quite often. Generally speaking, we have no idea of the probability of failure at any given point, though we may be able to discern specific cases in which the software will fail. All we can do to reduce the failure rate is test thoroughly and practice good software development. But logically, we distribute flawed software all the time. Would it make you uncomfortable as a developer to know _how often_ your software might fail, but to know nothing about the specific cases in which it might? What failure rate is acceptable? 0.01%? 0.0001%? Would you hold a genetic algorithm to a higher standard than a human when it comes to failure rates?