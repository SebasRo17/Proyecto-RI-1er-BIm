We have a bigger application server which customers would like to have deployed locally. It consists of an MySQL server database, a REDIS database, multiple Web servers for sub parts, a NGINX reverse proxy so that these web servers are reachable from Port 80 and a homegrown C++ server. All sub parts (DB, Webservers, ...) have to be configured to be accessible by each other. At the moment (for in house use) deployment and configuration is done by hand; but in order to roll it out to customers, we would like to have a Out-of-the- Box-Solution which gets assembled by some build script. What would you suggest?   1. Maintaining a VM which just gets configured and then deployed?   2. Maintaining some installation script or package   3. Something else? Usually we would prefer 2. as it seems the more natural way and let the customer decide whether he wants to use a real server or a VM. Also sounds automatic creation of VMs rather time-expensive. The Problems we face are the usually not-to-be-embedded-components like MySQL and NGINX. NGINX configuration is stored in /etc/nginx/. I don't think that .DEB-packages are allowed to overwrite foreign NGINX configuration nor is it a good practice. The same with MySQL. It is also possible to embed MySQL/NGINX/Redis but this is not a trivial task.