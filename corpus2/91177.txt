I have a requirement to filter profanity out of users' submissions in a Java- based web application. The client is aware of both the Scunthorpe Problem and the Clbuttic Problem and have accepted the consequences. Please, I don't desire a debate on the merits of lack thereof of censorship. There are two bits of data:   1. The user's submission, which can potentially contain 500 words or so;   2. A single-column database table containing words that are disallowed. There may be many thousands of records in this table. The present solution seems wrong to me:   1. The entire table is loaded into a static String[] on startup into a Singleton (thus residing in memory).   2. For each user submission we loop through the array and do a .indexOf() to see if any given word in the String[] appears in the submission.   3. If it appears, we replace with %$#@%-style characters. This is done by tokenizing the user submission, looping through the entire user submission as tokens (again), and replacing each instance of the found word. There may be brilliance in this solution, but I'm skeptical. And having looked at it for a while I can't find my way past it. Questions is, what is a solution that will give good performance and hopefully be reasonably sane for future developers to maintain after I get fired for failing to filter out some obscure word I've never heard of?