For several years now I am a big fan of using static code analysis tools for checking the source code quality. We are mostly doing C# development so NDepend was the best way to go for me. Most of the time I manually perform the analysis, for example once every two weeks. Lately I have had a discussion with a college whether we should include NDepend in our continuous integration process so that the build automatically turns red if for example a method has a too high complexity. My opinion hereby is that it's not possible to define metrics that are valid for the whole code. I think that you would either get way to many "false complaints" - code that generally is ok but does not match the metrics. The following code for example has a very high complexity, even if I think that sometimes you can't prevent it.               if (remoteResult == RemoteResult.Dummy1 || remoteResult == RemoteResult.Dummy2 ||          remoteResult == RemoteResult.Dummy3 || remoteResult == RemoteResult.Dummy4 || ...      Of course it would also work to increase the thresholds of the metrics, but in this case I would loose valuable information. Another example for example is the maximum number of members for a class. I would like to keep this lower than 10 for classes with lots of logic. But on the other hand there are many "domain models" with more than 10 properties. I wouldn't like to extract another class just because of the metric - as this would lower the readability and does not improve it which should be the goal of code quality metrics. Do you think automatically checking code quality via metrics is possible? If yes, for all or only for specific metrics? (A list of common NDepend metrics can be found at http://www.ndepend.com/Metrics.aspx )