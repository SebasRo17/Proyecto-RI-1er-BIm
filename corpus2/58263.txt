I'm playing with integer linear programming, especially problems involving lots of constraints which would appear to be special ordered sets. Below is a toy example, three last rows of the matrix defining SOS1-like constraints.               LinearProgramming[       (* use SOS dummy variables for weight *)       ConstantArray[-1, 3] ~Join~ ConstantArray[0, 9],            {         (* 3 SOS dummy variables in the beginning *)         (* blocks: {1,0,1}, {2,1} and {1,0,0,1} with all alignments to 5-item array *)         {0,0,0,1,0,0,2,0,0,0,1,0},         {0,0,0,0,1,0,1,2,0,0,0,1},         {0,0,0,1,0,1,0,1,2,0,0,0},         {0,0,0,0,1,0,0,0,1,2,1,0},         {0,0,0,0,0,1,0,0,0,1,0,1},         (* number of alignments per each block present in a solution *)         {0,0,0,1,1,1,0,0,0,0,0,0},         {0,0,0,0,0,0,1,1,1,1,0,0},         {0,0,0,0,0,0,0,0,0,0,1,1},         (* weights for dummy variables: ordered set *)         {-1,0,0,3,2,1,0,0,0,0,0,0},         {0,-1,0,0,0,0,4,3,2,1,0,0},         {0,0,-1,0,0,0,0,0,0,0,2,1}       },            (* limit sum of each position in 5-item array to 2 or less *)       ConstantArray[{2,-1}, 5] ~Join~        (* limit at most one shift per block to be present: SOS1 *)       ConstantArray[{1,-1}, 3] ~Join~        (* constrain SOS dummy variables *)       ConstantArray[{0,0}, 3],            Automatic, Integers]      > {3, 3, 2, 1, 0, 0, 0, 1, 0, 0, 1, 0} Many linear programming solvers provide explicit support for SOS, since their properties allow solvers to find solutions more efficiently. My questions are: does `LinearProgramming` have heuristics for efficient exploitation of SOS, and if it has some that are more efficient than naive presentation in the example above, how to take advantage of them? As often with neatly abstracted interfaces in Mathematica, it's practically impossible to reason about such optimizations in `LinearProgramming`.