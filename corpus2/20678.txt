There is a function available from the Statistics`Library context called NConditionalEntropy that appears to compute ConditionalEntropy. Thus ...               Statistics`Library`NConditionalEntropy[{1, 1, 1, 0, 1, 1, 0, 0}, {1,      0, 0, 1, 0, 1, 0, 0}]      outputs 0.954434 When I look at the definition of Conditional Entropy in Wikipedia (http://en.wikipedia.org/wiki/Conditional_entropy), it suggests that it is the expectation of the base 2 log of the PDF of a marginal distribution of p[x,y] that it calls p[x] divided by the PDF of the distribution p[x,y], where the results are weighted according to this same distribution p[x,y]. So, this gave me hope that I could recreate ConditionalEntropy as an expectation and see if I really understood what was going on. Thus, I write the following code:               xv = {1, 0, 0, 1, 0, 1, 0, 0, 1};(* just some test data*)     yv = {1, 1, 1, 0, 1, 1, 0, 0, 1};(* just some test data*)     ed = EmpiricalDistribution[     Flatten[Outer[     List, xv,yv], 1];     Expectation[     Log[2, PDF[MarginalDistribution[ed, 1], x]/     PDF[ed, {x, y}]], {x, y} \[Distributed] ed]//N      And I get 0.918296 But when I write ...               Statistics`Library`NConditionalEntropy[xv, yv]      I get 0.899985 In case I've got the order of arguments wrong, I've also tried               Statistics`Library`NConditionalEntropy[yv, xv]      But this yields 0.972765, which still does not match up. Several theories for the discrepancy: 1) I do not understand the concept of Conditional Entropy well enough 2) My code for implementing conditional entropy is missing something 3) Other Help appreciated.