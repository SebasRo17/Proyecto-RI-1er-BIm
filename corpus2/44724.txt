I'm toying around with LLVM and looking at what it would take to make yet another strongly-typed language, and now that I'm around the syntax, I've noticed that it seems to be a pet peeve of strongly typed language to warn people that their real constants won't fit inside a `float`:               // both in Java and C#     float foo = 3.2;     // error: implicitly truncating a double into a float     // or something along these lines      Why doesn't this work in Java and C#? I know it's easy to add the `f` after the `3.2`, but is it really doing anything useful? Must I really be _that_ aware that I'm using single-precision reals instead of double-precision reals? Maybe I'm just missing something (which, basically, is why I'm asking). Note that `float foo = [const]` is not the same thing as `float foo = [double variable]`, where requiring the cast seems normal to me.