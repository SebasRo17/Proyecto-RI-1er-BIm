I'm going to be developing some functionality that will crawl various public web sites and process/aggregate the data on them. Nothing sinister like looking for e-mail addresses - in fact it's something that might actually drive additional traffic to their sites. But I digress. Other than honouring `robots.txt`, are there any rules or guidelines, written or unwritten, that I ought to be following in order to (a) avoid appearing malicious and potentially being banned, and (b) not cause any problems for the site owners/webmasters? Some examples I can think of which may or may not matter:   * Number of parallel requests   * Time between requests   * Time between entire crawls   * Avoiding potentially destructive links (don't want to be the Spider of Doom - but who knows if this is even practical) That's really just spit-balling, though; is there any tried-and-tested wisdom out there that's broadly applicable for anybody who intends to write or utilize a spider?