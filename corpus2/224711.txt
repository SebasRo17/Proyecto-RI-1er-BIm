When developing features with TDD, I create a test for each combination of feature and case. So, one test for creating user successfully, one for validation errors, and one for database errors. I don't test each one with many different types of data, in general, as I write the tests to flesh out features, less to catch errors. I don't think it's time-effective to write out several cases (even with a "data provider") for every single feature I develop in order to validate that the feature works correctly in all cases. Sometimes, though, there are certain implementation specific bugs to be wary of when implementing the feature. After writing the feature test that allows me to write the implementation, I'll notice that certain edge cases need to be tested more carefully because of the implementation. How does one organize these tests with the rest of the feature-specific tests? Are the tests I'm describing regression tests? **Edit:** I guess I wasn't clear about what I meant with "different types of data" and "works well in all cases." My main issue is in writing implementation-specific tests--specifically the workflow and organization. We _are_ testing the interface. We're specifically testing the feature only, at first. But a certain implementation requires more tests.