In the following `a`,`b` is a probability distribution, i.e. `a+b==1` with `a`,`b` real, and in the interval `[0,1]`. If we try to maximize the Shannon entropy over this distribution using Mathematica, we can either set `b=1-a`, or state the total probability condition as an assumptions. Maximizing using `b=1-a` works as expected. This input `Maximize[{-Log[2, a]*a - Log[2, 1 - a]*(1 - a), a <= 1 && a >= 0}, a, Reals]` yields `{1, {a -> 1/2}}`. However, when we give `Maximize[{-Log[2, a]*a - Log[2, b]*b, a + b == 1 && a >= 0 && a <= 1 && b >= 0 && b <= 1}, {a, b}, Reals]` as input, Mathematica does not find the solution. Any suggestions how the later input can be altered to find the correct solution?