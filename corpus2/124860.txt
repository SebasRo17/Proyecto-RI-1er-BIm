Some code is written to generate Excel Spreadsheets (Office Interop).   * The code performs very poorly.       * A subsystem is designed to generate the files at night. Performance isn't a concern at night.        * A function is created to pick the correct file from the 100 different files available depending on a chosen set of parameters.       * Because physical files exist, an archival system is added to backup these files (There is no reason to archive. These files should be generated on the fly).       * This system doesn't include a configuration file, instead it has a hard coded "server picker" function that simply reflects upon the server the code is running upon.       * A scheduled task is necessary to support and run this service. This boils down to a single problem. The original code performs far too poorly to run in a production environment. Had the performance problem been resolved, the subsystem and subsequently archiving system, "file picker factory function", hard coded failure point and the maintenance of the scheduled task and its added point of failure have no need to exist. This is a "cascading failure" if you will. The original problem led to more bad code, more bad solutions and unnecessary overhead. Is there a formal anti- pattern or general term to describe it?