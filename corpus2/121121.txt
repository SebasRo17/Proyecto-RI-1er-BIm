Let's say I have a method:               public void DoSomething(ISomeInterface someObject)     {         if(someObject == null) throw new ArgumentNullException("someObject");         someObject.DoThisOrThat();     }      I've been trained to believe that throwing the `ArgumentNullException` is "correct" but an "Object reference not set to an instance of an object" error means I have a bug. Why? I know that if I was caching the reference to `someObject` and using it later, then it's better to check for nullity when passed in, and fail early. However, if I'm dereferencing it on the next line, why are we supposed to do the check? It's going to throw an exception one way or the other. **Edit** : It just occurred to me... does the fear of the dereferenced null come from a language like C++ that doesn't check for you (i.e. it just tries to execute some method at memory location zero + method offset)?