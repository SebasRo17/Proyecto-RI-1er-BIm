In my line of work I deal with very large files, hundreds of Gigabytes in size. The nature of the data in these files is such that compression would greatly reduce their size. The problem is, the records/data packets within the file must be individually accessible. Is there a way to apply some home-grown techniques to these records to individually compress them and, once compressed, place them in a data stream such that the byte offset locations of each compressed packet is still known? Would this kind of fragmentation of the packet data substantially affect the efficiency of the compression/decompression cycle? Are zip algorithms suitable for this, or are there better methods of compression that are designed specifically for this?