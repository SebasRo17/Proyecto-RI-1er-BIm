I'm upgrading my Mac to one of the new Mac Pro R2D2 Darth Vader models that have just been released. They come standard with twin AMD FirePro D300 cards, or twin D500 cards. I don't need that much graphics processing power, and since I understand the D500 puts out a lot more heat than the D300, and I really don't like fan noise ... I was going to go for the D300s. However, Apple's Mac Pro blurb says: > The FirePro D500 supports fast double-precision computations, executing at > one-quarter the performance of single-precision floating point rather than > the 1/16 performance seen in the D300 and most consumer GPUs And this got me wondering: If GPUs support double precision numerics, is it possible currently (or in the works??) for _Mathematica_ to AUTOMATICALLY take advantage of the massive teraflop GPU power for numerical computations? I don't want to have to specially load CUDALink libraries or to write special code or to specially load OpenCL functions … I just want to use standard _Mathematica_ functions. In brief: Can _Mathematica_ automatically take advantage of the new Mac Pro GPU for numerical calculations? If not, I think I'll stick with the quieter, cooler D300 GPU, … and max something else out! * * * Addendum: Apple's new Autovectorizer in OSX Mavericks https://developer.apple.com/library/mac/documentation/Performance/Conceptual/OpenCL_MacProgGuide/AutoVectorizer/AutoVectorizer.html#//apple_ref/doc/uid/TP40008312-CH7-SW1 > Using OpenCL is easier than ever as of OS X v10.7: The autovectorizer allows > you to write one kernel that runs efficiently on both a CPU and a GPU. You > can invoke the autovectorizer regardless of whether you are compiling from > Xcode or building the kernels at runtime.