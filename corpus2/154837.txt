Stack based virtual machines like CLR and JVM has different set of instructions. Is there any theory behind devising the instruction set while creating a virtual machine? e.g. there are JVM instruction sets to load constants from 0-5 onto the stack               iconst_0     iconst_1     iconst_2     iconst_3     iconst_4     iconst_5      whereas in CLR there are instruction set to load number from 0 to 8 onto the stack as follows               ldc.i4.0     ldc.i4.1     ldc.i4.2     ldc.i4.3     ldc.i4.4     ldc.i4.5     ldc.i4.6     ldc.i4.7     ldc.i4.8      why there is no ldc.i4.9 and if ldc.i4 is there why we need the above opcodes? And there are others like these. I am eager to know what is the reason behind this difference between opcodes of different VMs? Is there any specific theory to devise these opcodes or it is totally driven by the characteristics of the VM itself or depends on the high-level language constructs?