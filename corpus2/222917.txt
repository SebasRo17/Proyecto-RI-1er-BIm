I am working on a list comparator to assist sorting an unordered list of search results per very specific requirements from our client. The requirements call for a ranked relevance algorithm with the following rules in order of importance:   1. Exact match on name   2. All words of search query in name or a synonym of the result   3. Some words of search query in name or synonym of the result (% descending)   4. All words of the search query in the description   5. Some words of the search query in the description (% descending)   6. Last modified date descending The natural design choice for this comparator seemed to be a scored ranking based on powers of 2. The sum of lesser important rules can never be more than a positive match on a higher importance rule. This is achieved by the following score:   1. 32   2. 16   3. 8 (Secondary tie-breaker score based on % descending)   4. 4   5. 2 (Secondary tie-breaker score based on % descending)   6. 1 In the TDD spirit I decided to start with my unit tests first. To have a test case for each unique scenario would be at a minimum 63 unique test cases not considering additional test cases for secondary tie breaker logic on rules 3 and 5. This seems overbearing. The actual tests will actually be less though. Based on the actual rules themselves certain rules ensure that lower rules will always be true (Eg. When 'All Search Query words appear in description' then rule 'Some Search Query words appear in description' will always be true). Still is the level of effort in writing out each of these test cases worth it? Is this the level of testing that is typically called for when talking about 100% test coverage in TDD? If not then what would be an acceptable alternative testing strategy?