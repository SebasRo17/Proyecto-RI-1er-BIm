I have a database containing just simple URLs. It is as simple as it sounds for now and URL can link to a website or a document(i.e. anything parseable to text). Now I have a simple code which inserts records to database. The problem is that website/document may be actually the same, just:   * Hosted somewhere else   * Not available so linked from Google Cache   * Not available so linked from archive.org   * Page can be republished from another source   * etc... _**I would like to get some kind of fingerprint of a website/document and think of a way to do so._** What I have thought of: **I can rely on title** because even if content is published somewhere else or cached on some server it will probably will have the same title. That's ok, as title is usually short and doesn't consume a lot of space. Downside - this works only on website. Maybe a filename is appropriate for documents but these can be renamed too. **I can rely on keyword count** But the problem is that I just enter URL and no keywords nor do I want to do so. It is a simple URL storage engine. **I can get a some kind of checksum of all the content** But this method would be a total guess. **SO MY QUESTION IS:** How can I fingerprint a content so later on I could identify possible duplicates ? **EDIT** I don't want to fingerprint just the title. I want to fingerprint the whole content. Content stays the same but can be hosted anywhere and it's structure(title too) can change. For documents - filename can change too. I want to fingerprint content. All the text so I can later on identify possible duplicates.