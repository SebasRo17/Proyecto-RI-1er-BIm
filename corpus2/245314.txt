I need help in identifying a better algorithm. I have developed a script using pythons scipy package to analyse a rather large model that I wish to solve. The model contains over 12GB of data including over 500 parameters. The problem is that when I run small simulations of about 0.5GB of data with 20 parameters it can take my computer a decent amount of time if I allow a reasonable number of iterations through the random forest classifier. Currently my script is only using one core, so I guess that making the script multi-threaded would be the first step. But I do not believe this will be enough given how complex the model is. I am willing to explore the use of a cluster based HPC solution but I am not sure how to go about this. Are there better algorithms I can use, or is there a cluster based algorithm that would be more appropriate?