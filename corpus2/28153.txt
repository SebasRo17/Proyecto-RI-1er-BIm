I'm having trouble understanding a slowdown associated with code I'm running, which should be trivial to parallelize. My code looks something like this:               DistributeDefinitions[Movie];          List =        ParallelTable[        ComponentMeasurements[          {            MorphologicalComponents[EdgeDetect[Movie[[frame]]], Method -> "ConvexHull"],             Movie[[frame]]          },           "IntensityCentroid"][[All, 2]],         {frame, 1, 100}];      This works, but takes about 14.265 seconds to run on 12 cores. However, changing "ParallelTable" to just "Table" reduces the running time to about 2.791. Changing {frame, 1, 100} to {frame, 1, 10} still yields the same result that "Table" runs about 5x as fast as "ParallelTable". The "Movie" data set is a few gigs in size, but shouldn't that not matter if I'm applying "DistributeDefinitions" to it? * * * For a self-contained example, we can use the "Mandrill" example data image in Mathematica 9:               Movie = Table[ExampleData[{"TestImage", "Mandrill"}], {x, 1, 5000}];     DistributeDefinitions[Movie];     ParallelTable[       ComponentMeasurements[         {           MorphologicalComponents[EdgeDetect[Movie[[frame]]], Method -> "ConvexHull"],            Movie[[frame]]         },          "IntensityCentroid"][[All, 2]],        {frame, 1, 20}];      Running "ParallelTable" here takes about 6.85 seconds of time on a 12 core machine. Running "Table" 7.24 seconds. However, changing the number of frames in the movie from 5,000 to 10,000               Movie = Table[ExampleData[{"TestImage", "Mandrill"}], {x, 1, 5000}];      to               Movie = Table[ExampleData[{"TestImage", "Mandrill"}], {x, 1, 10000}];      yields the same `Table` computation time of 7.26 seconds and increases the `ParallelTable` computation time to 9.71 seconds.