I am doing some investigating and research into programming "big data", and am particularly interested in QuantCast (QFS) because it is written in my own favourite programming language, C++. Looking at how this may play a part in the solution to certain "big data" problems, I know this can manage implementation of peta-byte file systems. I would like to know however how scalable it is for big-data systems that are not as hugely big. I have seen a lot of comparisons between QFS and Hadoop, and how QFS can outperform Hadoop and so anyone who uses Hadoop may consider using Quantcast (if they just want mega performance). Essentially is it scalable for medium to large projects? (Someone asked the same about BDD but not Quantcast) (Also I have seen Hadoop and thus Quantcast being called a "database". I would call it a filesystem, on top of which you might build a database but wouldn't call it a database itself).