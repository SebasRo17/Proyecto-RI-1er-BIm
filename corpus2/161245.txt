How does caching effect the performance of a running program? From my understanding, the assumption that each instruction always takes the same amount of time is not always correct, because of the effects of caching. How does caching affect algorithmic performance? Thanks in advance. **EDIT 1:** To further elaborate, lets say I'm running this Java program that's reading one million integers. From what I understand about this program, it's order of growth is cubic and will take a while to finish running. How would caching improve the performance of this program while it is running?