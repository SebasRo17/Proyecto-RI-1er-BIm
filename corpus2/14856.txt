"Best practices" are everywhere in our industry. A Google search on "coding best practices" turns up nearly 1.5 million results. The idea seems to bring comfort to many; just follow the instructions, and everything will turn out fine. When I read about a _best practice_ \- for example, I just read through several in Clean Code recently - I get nervous. Does this mean that I should _always_ use this practice? Are there conditions attached? Are there situations where it might _not_ be a good practice? How can I know for sure until I've learned more about the problem? Several of the practices mentioned in _Clean Code_ did not sit right with me, but I'm honestly not sure if that's because they're potentially bad, or if that's just my personal bias talking. I do know that many prominent people in the tech industry seem to think that there are no best practices, so at least my nagging doubts place me in good company. The number of best practices I've read about are simply too numerous to list here or ask individual questions about, so I would like to phrase this as a general question: **Which coding practices that are popularly labeled as "best practices" can be sub-optimal or even harmful under certain circumstances? What are those circumstances and why do they make the practice a poor one?** I would prefer to hear about specific examples and experiences.