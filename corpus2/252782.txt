While reviewing another programmer's implementation of a function to calculate the normal distribution CDF, I made a suggestion to either replace the entire implementation with Python's built-in functions or use SciPy, a common scientific library. Another programmer pointed out that neither `math.erfc()` nor `scipy.stats.norm.cdf()` provides any precision guarantees in their documentation. Therefore, I should be more cautious about replacing an approximation algorithm (which was taken from a respected source, and which had documented error bounds). To be honest, the thought to doubt the accuracy and precision of a built-in or library function had never crossed my mind. After all, I've been calling functions like `sin()` and `sqrt()` for years without much thought — why should `math.erf()` or `scipy.stats.norm.cdf()` be any different? But now, I'm concerned. My questions are:   1. In general, if the documentation makes no special mention, is it implied that these kinds of functions are completely accurate to the last decimal place, within the precision offered by IEEE double-precision floating-point?   2. Is that true for Python's `math.erf()` or SciPy's `scipy.stats.norm.cdf()` in particular? How can you tell?   3. This man page for `sin()` says… > These functions may lose accuracy when their argument is near a multiple of > pi or is far from 0.0. Why should such caveats exist, when the sine function is periodic and symmetrical? There seems to be a burden placed on the caller to canonicalize the input to obtain optimal accuracy. On the other hand, Mozilla's documentation for `Math.sin()` says nothing about accuracy or precision. Does that mean that it is fully accurate, or is it "common knowledge" that `Math.sin()` would only be accurate in certain circumstances in JavaScript, like everywhere else?