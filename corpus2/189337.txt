We've recently worked with a mathematician to build us an algorithm. The algorithm will look at click data and will continuously update data associated with the user, the content, and the content's category and then pair the user with relevant content. With that said, I have never implemented an algorithm before but I'm guessing that our current environment (PHP, MySQL) is not entirely suitable for continuously crunching and updating data. **Looking for feedback on whether the following approach is on the right track?**   * Write algorithm in Java (or other compiled language) for best performance.   * Store user, content, and category data on a NoSQL server (or use memcache).   * Use Gearman (or equivalent) to submit click/user data to a job server.   * Run jobs on separate worker server that contains the algorithm.   * Update user, content, category data. If this is not on the right track, can you explain why would that be?