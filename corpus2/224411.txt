Suppose I have a function. I have code that, given an x value, produces a y value. I can assume that the function is somewhat mathematically sane, not something crazy like an everywhere-discontinuous function or the like, but beyond that, I don't know anything about it. I want to draw a graph that is likely to make sense to someone looking at it. My boundaries for x and y values are fixed. Obviously, I can put in a bunch of x values, get the corresponding y values, and play connect the dots. But that will produce misleading answers in some cases. For example, the graph of y=1/x has a discontinuity at x=0. If I play connect the dots, and zero does not happen to be one of my input x values, I'll draw a line across the discontinuity, instead of having y go up to infinity as x approaches zero from above, then reappear from minus infinity for slightly negative x. It seems that I might want to be suspicious whenever the direction of change of the y values reverses itself. In other words, mistrust apparent peaks and valleys; look at some more points to see what is really going on. But that could fail, too; consider the function y=100x+1/x. Now, unless my sample points get very close to zero, I will still miss the asymptote. Someone must have studied this problem. Is there any relevant literature? If so, where?