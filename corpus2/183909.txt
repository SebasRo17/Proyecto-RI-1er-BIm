I'm working with an embedded device with a unique protocol that sends messages to other devices and I'm making an application that parses the sent packets. Each packet carries 8 bytes. The protocol is defined as where first byte is header and remaining 7 bytes is the data. They are trying to pass a particular ID string but the ID string is 8 characters long (ASCII) so it won't fit in 7 bytes. What my colleague told me is that they're going to turn the 8 ascii bytes of original string into integer (decimal) and send me 4 bytes of it. They told me I should be able to get the original string from the 4 bytes. I'm having a hard time wrapping my head around on this. So if you have an ID string like "IO123456", that's 0x49 0x4f 0x31 0x32 0x33 0x34 0x35 0x36 in ASCII.. How on earth can you compress that in 4 bytes by turning it into an integer and I can get the original string from it? Am I missing something or is my colleague mistaken? I understand this is a really bizzare question but this seriously does not make any sense to me.