Consider these Fourier transforms               FourierTransform[Exp[-I ω0 t] (Exp[-(t)^2]), t, ω]; // AbsoluteTiming     FourierTransform[Exp[-I ω0 t] (Exp[-(t/T0)^2]), t, ω]; // AbsoluteTiming     FourierTransform[Exp[-I ω0 t] (Exp[-((t + τ)/T0)^2]), t, ω]; // AbsoluteTiming     FourierTransform[Exp[-I ω0 t] (Exp[-((t - τ)/T0)^2] + Exp[-((t + τ)/T0)^2]), t, ω]; // AbsoluteTiming          (*{0.022893,Null}*)     (*{0.167016,Null}*)     (*{0.051487,Null}*)     (*{0.183451,Null}*)      It's pretty fast, but if I setting the assumption, then it becomes much slower               $Assumptions = T0 > 0 && ω0 > 0 && τ > 0;          FourierTransform[Exp[-I ω0 t] (Exp[-(t)^2]), t, ω]; // AbsoluteTiming     FourierTransform[Exp[-I ω0 t] (Exp[-(t/T0)^2]), t, ω]; // AbsoluteTiming     FourierTransform[Exp[-I ω0 t] (Exp[-((t + τ)/T0)^2]), t, ω]; // AbsoluteTiming     FourierTransform[Exp[-I ω0 t] (Exp[-((t - τ)/T0)^2] + Exp[-((t + τ)/T0)^2]), t, ω]; // AbsoluteTiming          (*{0.018221,Null}*)     (*{6.465289,Null}*)     (*{9.765310,Null}*)     (*{48.956260,Null}*)      Why does the performance decrease a lot? And is there a way to use the global assumption while maintain the performance?