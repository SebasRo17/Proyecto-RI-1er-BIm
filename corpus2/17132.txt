I run a heavy simulation, using `NDSolve` and the method of lines. This means that the result of `NDSolve` is a list of, say, 400-1000 interpolation functions with many interpolation points. The whole thing can weigh up to 4GB. As it is, this poses no problem. The challenge is when I try to make a movie out the simulation. I do the following: I have a function               snapshot[sol_,t_]:=GraphicsGrid[{Plot[...],Plot[..],...}]      which take the result of `NDSolve` (what I call `sol`) and a specific time `t`, and makes some plots of the simulation at that time. Then, I export these graphics with something like               Table[       Export["~/tmp/" <> IntegerString[k, 10, 4] <> ".png",snapshot[sol, tList[[k]] ],     {k, 1, n}]      where `tList` is a list of times. Then I can make a movie out of the images with an external program (`ffmpeg`). The problem is that exporting the graphics like this takes a lot of time and I want to run it in parallel. Using `ParallelTable` is very bad, because it creates a local copy of the very heavy variable `sol` in all subkernels, and this immediately freezes my computer. I had no luck with `ParallelSubmit` or the like. Any ideas? _**EDIT**_ In response to all the commenters:   1. The plot is indeed local in the time variable.   2. I tried converting the `InterpolatingFunction`s to an numerical array. It saves some memory (a factor of ~50) but that's not enough - Still if you duplicate the data 1000 times (for 1000 frames) you'll be in bad shape. Basically, what I want is to send requests to **multiple processors** , but that will all run **on the same kernel**. This way you don't have to duplicate I thought it was possible because in MATLAB I do that naturally with the `parfor` command (or, at least, I think that's what I'm doing).