I'm building a framework to work as an all-purpose astronomy pipeline and before I get too far into development I was hoping to run my needs by you all to see if there are any optimizations or pitfalls I'm missing. This is a long one, so thank you to anyone who takes the time to look this over. My main needs are as follows (in order):   1. Cross platform and easy to install without a lot of dependencies. Most of the astronomers I work with are very good at understanding physics and astronomy but are weaker in programming ability and due to time constraints they are unlikely to adopt a new solution to reduce their data unless it's lightweight and easy to install on their choice of OS. If something looks complicated to install or setup then they will likely look for something else or continue using the decades old software they are currently using. I also like using python since a growing number of people in the astro community are using it.   2. Centrally located on a server. Currently most open source solutions for reducing data sit on the users machine. This means each user in a group must download the same data from the server, wasting disk space and time (often file sizes can be O(100Mb) or more). It also means they have to install and setup a variety of programs written by different people and it can take weeks to get one machine fully configured... until the next software update when something breaks and the procedure must be started again. So I want to use a web based solution where the images will all be located on a server that does the majority of the processing and my application is basically just a UI in a web browser to interact with all the nastiness that is open source astronomy command line software.   3. Ability to run both short and long runtime functions. For example, I have a viewer that converts a fits image into a bunch of png tiles and sends them to the client to be viewed on a canvas. This must be done as quickly as possible. Other tasks will run routines like photometry or data reduction that may take an hour or more, depending on the task and dataset. So it is important to be able to have some routines run asynchronously (like building and loading images) while other tasks are queued (like finding all the bright and isolated stars in a given image).   4. Modular structure. I plan on building a basic UI framework in python that can easily be extended by users. This will allow the community to share more of their code and only install the pieces needed for an individual or groups research. Functions essentially become webpages with an image viewer, instructions, and an interface for parameters to minimize the time it takes for new students to learn to use the software.   5. Small group of users. If there are 10 people using a server concurrently that is usually a lot. But if possible it would be nice to have something that scales up rather easily so that larger projects could adapt this for their needs.   6. Processing on multiple cores. While not necessary, our groups server has more cpu's than members of the group. So it would be nice if each user could log in and have their own core to run their jobs without interfering with other users. So what I have begun to develop is a python tornado server with mongodb solution on the server. Clients connect to the server via a web socket and send jobs to be processed. The job server then runs the routine specified by the client and sends the result to the client, or sends a link if an image or large data file is ready to be downloaded. The database is optional but handy (this is the part I'm implementing now). Typically the image files we deal with contain a header with 100 or so keywords. So when a new set of images is downloaded the user can load the headers into the database (this could be 100's of files). For a number of reasons, the user may want to search the files for any one of the hundred keywords, not just a few key indices. When I dump the header data into a cpickle object it's over 100mb and takes a minute or so to load. I'm hoping that mongodb (or some other db with a python interface) is faster and will allow for multiple users to access the data concurrently, but I've learned that databases can be very different and performance can vary greatly depending on the need. An additional function of the database will be to keep track of user accounts and which directories of images they are allowed to see (for example a collaborator from another university may have access to a set of data taken together while other datasets are for internal use only). One thing I've thought of doing is writing a general db api. Since most of the searches will be fairly simple functions could just call a getFiles routine that allows them to input lists of selection criteria and then the user simply needs to add the correct syntax for his/her particular db in the get function. Perhaps this is overkill, but one of the main things I'm trying to combat is the tendency in astronomy to use outdated code and technologies because no one wants to take the time to update them. My hope is that by making this as flexible as possible it can easily change as new tools are available (such as a better suited web server, database, or even more commonly used programing languages, keeping in mind that many of my collaborators still use fortran 77 code written before I was born). I've also considered running multiple job servers, one for tasks that need to be quickly accessed, and anther for jobs with long run times. But it would be nice to have a single server solution. I've put about a month into building the framework so far and unless there is a very good reason to switch web application servers I'd like to stick with Tornado. It's lightweight, easy to use, and seems to meet all of my needs. I just started looking at databases so I'm much more flexible and open to other options here. Thank you to anyone who has read through or skimmed most of this, any advice you have is greatly appreciated.