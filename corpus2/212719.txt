Looking at Euclid's algorithm for the "Greatest common divisor of two numbers", I'm trying to divine the big-O cpu time for numbers K, and N. Can anybody help? This is the algorithm as I understand it.. Where:   * max(A,B) = the greater of A or B such that: min(10,3) = 10   * min(A,B) = the smaller of A or B such that: min(10,3) = 3   * modulus(A,B) = the remainder of A divided by B such that: modulus(10, 3) = 1 The algorithm is:   1. r = modulus(max(K,N), min(K,N))   2. if r = 0      * then GCD is max(K,N)     * else:        * max(K,N) = r       * go to step 1 It appears to me that due to the division occurring, it can't be a linear algorithm which is why it's useful as something more efficient than a naive implementation that just tries all the possibilities from 0 to min(K,N). But I can't quite figure out just what the runtime is. Any pointers would be very helpful!