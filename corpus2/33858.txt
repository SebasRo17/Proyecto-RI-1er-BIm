I have $389$ data points defined in an `EventData` object and want to judge the quality of fit of this data to a fitted $3$-parameter Weibull distribution. This particular data set contains no censoring but in general there will be censoring in my future data sets. My approach is to find the three parameters using `EstimatedDistribution` and then to represent the found distribution by generating a number of random variates from it so that they can be compared to the original `EventData` object. I’m using LogRankTest to produce the $p$-value to judge the fit. My fundamental question is, is there a more elegant way of doing this in _Mathematica_? My data and code are below. Here are the raw data:               data1=Table[40.8,{10}];     data2=Table[42.075,{23}];     data3=Table[43.35,{48}];     data4=Table[44.625,{80}];     data5=Table[45.9,{63}];     data6=Table[47.175,{65}];     data7=Table[48.45,{47}];     data8=Table[49.725,{33}];     data9=Table[51,{14}];     data10=Table[53.55,{6}];     data=Flatten[{data1,data2,data3,data4,data5,data6,data7,data8,data9,data10}];      Here is where I define the `EventData` object and determine the fitted distribution, distribution.               censoring=Table[0,{Length[data]}];     eventdata=EventData[data,censoring]     dist=EstimatedDistribution[eventdata,WeibullDistribution[a,b,c]]          EventData[ ]          WeibullDistribution[2.62045,7.14193,39.7656]      And here is where I non-parametrically determine the $p$-values via `LogRankTest`.               n=200000;     i=0;     Do[      {i=i+1,      If[i>4,Break[]],      samplevalues=RandomVariate[dist,{n}],      logrankpvalue=LogRankTest[{eventdata,samplevalues}] ,      Print["i=",i,"    Log Rank p Value=",logrankpvalue]},      {5}]          i=1    Log Rank p Value=0.999826          i=2    Log Rank p Value=0.973828          i=3    Log Rank p Value=0.948566          i=4    Log Rank p Value=0.976803      Note that with $n$ set at a relatively large value ($200\,000$), $p$-values of around $0.97$ are produced indicating that the hazard rates of the two data sets are in pretty good agreement. I think this tells me that `EstimatedDistribution` did a good job in the fitting. Conversely, in general, small values of $n$ (e.g., $10$) produce a much poorer agreement as expected and as evidenced by smaller $p$-values. I generate four $p$-values because I’ve observed that depending upon the particular random values chosen that it is possible to produce a large $p$-value with small values of $n$. My questions are   1. Is there a more elegant way of determining the degree of similarity between the `EventData` object and the found distribution? Perhaps comparing to the theoretical distribution, dist directly instead of sampling from dist?   2. If this approach (or any improved suggested approach) is robust for the _no-censoring_ case is it also robust if the `EventData` object contains _censored values_?