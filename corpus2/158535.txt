I'm working at a company that would score 11 on Joel Test - at least on paper. In practice, however, nothing works quite as well as expected, and the project has been on **DEFCON 1** for half a year. Now, most of my peers are happy if they can go back home at 6pm - on Sunday. One of the apparently good practices that struck me as not working is the use of static analysis tools. The project both tracks gcc -Wall warnings and a proprietary and very expensive _"C/C++"_ tool. Gcc warnings do more often than not point to real(if most of the time inoffensive) bugs. The proprietary tools, however, list things such as implicit casts and sizeof'ing a string literal. Implicit casts are also blacklisted on their stylebook. The standard practice is that people is pressed to make every single warning shut up. Note that this does exclude warnings that are predominantly false positives, this is not the problem. The result is:   1. People add type casts to every rvalue and to every argument hiding real problematic type mismatches in the process.   2. People introduce off by one bugs, or use a different problematic language feature.(strlen instead of sizeof, strncpy instead of strcpy, etc.)   3. The warnings are silenced.   4. The bug reports start rolling in. The main point is the original code was working and written by people who were playing safe within their language abilities whereas the fixes were not. Now, I don't really think this company can be saved. However, I would like to know if there is a better, preferably working, way to use the _"pro"_ tools or if I should just avoid using them altogether in case I am the one making the decision in the future. A solution which doesn't assume all programmers are geniuses that can't err. Because well, if they are, then there is no need to use the tools in the first place.