Having messed around with several scripting languages and being a bit of a linguist, there seems to be a way to divide dynamically typed languages into two groups: languages that give variables a default value, and languages that treat accessing an up-to-now unused variable an error. For instance, consider this code:               print(hi)        * In Lua, it prints `nil`   * In PHP (with a semicolon and `$`), it prints nothing   * In Perl (with a semicolon and `$`), it prints nothing (but using `say` yields an error) However,   * In Python it throws a `NameError`   * In Ruby, it throws a `NameError` (looks like Ruby ripped off Python's name for it)   * In Javascript, it throws a `ReferenceError`   * In Lisp, some type of error occurs that takes me to a debugger (I'm just learning Lisp now so I don't know what to call it) Additionally, these languages behave differently when indexing an hash map, a similar operation, and they again fall into two camps (and when I say 'nothing' I mean that language's representation of nothing):   * Lua returns nothing   * PHP returns nothing   * Ruby returns nothing   * Perl returns nothing   * Lisp returns nothing   * Javascript returns nothing But   * Python throws a `KeyError` So I am wondering what the comparative advantages and disadvantages are for a language to make accessing an undefined variable an error or just return a default value, and also what decisions would lead a language designer to choose one path or the other.