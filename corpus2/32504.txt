I'm writing a program similar to Python's `timeit` module. The idea is to time a function by executing it anywhere from 10 to 100,000 times depending on how long it takes and then report results. I've read that the most important number is the minimum execution time because this is the number that best reflects how fast the machine can run the code in question in the absence of other programs competing for processor time and memory. This argument makes sense to me. Would you be happy with this? Would you want to know the average time or the standard deviation? Is there some other measure that you consider more important?