Let's say we want to analyze running time of algorithms. Sometimes we say that we want to find the running time of an algorithm when the input size is n and for the worst possible case it is denote it by O(n). Sometimes though I see books/papers saying that we need to find the **expected time** of an algorithm. Also sometimes the **average running time** is used . What is "expected time"? In which cases is it useful to find _expected time_ instead of worst case time? **Edit** : I think there is a subtle difference between _expected running time_ and _average running time_ but I am not sure . Through this post I want to know the exact difference if any .