I'm looking for advice regarding a future development of mine, concerning a application that may be run in several machines that need to use and manipulate the same data. As this sounds somewhat vague let me elaborate on it. Picture a desktop application where the user manages information about clients, appointments, client history and what not. Now assume a handful of users using the same application in different machines need to use the same data mentioned. I believe the best way to tackle this situation would be storing the database in a central location, provided an API (REST for example) is in place to be used by every application instance. Assuming there could be about 5 to 10 application instances running requests on the database. Would this be the best way to go about providing low response time reply to requests by each instance? Does anyone know of something better or if this is the best way to go. What suggestions, advise, guidance, best practices, do and don't do would you tell me about. Thanks. **EDIT: Question clarification and its context on the application** The development will be composed of a client application that will connect to a server application, initially the client application will be a desktop application, however, future web and mobile client applications may be also developed if the user baseline demonstrates such desire. This is yet a basic sketch of how things might be since we still iron out the edges. The client application will be composed pretty much by the user interface and client server end to end communication. The server application will be devised in three layers:   * Service layer: This layer provides means by which the clients issue requests to the server and replies are sent back. Additionally this layer also acts as a client to other third party services like email (SMTP) and text message delivery (SMS Gateway) when clients request such actions.   * Logic layer: Business logic regarding requests issued by the clients is processed here.   * Data layer: Where database operations are performed. My doubt is about the service layer when connecting with client applications, I believe I should develop my own communication protocol with sockets and serialized objects to reduce the time it takes the client to reconnect with the server to issue new requests. The time I'm referring too is the time it would normally take for a regular HTTP request to perform an handshake before sending the request to the server like a regular REST API anyone would access via a web server. Should I develop my own connection protocol with a persistent socket connection and serialized objects being exchanged or go with a regular REST API. Some clients might send many requests one after the other for hours, but yet others might send a couple requests and that would be the end of it. I'm undecided on the path I should follow here, REST or a custom socket connection protocol like mentioned before? **UPDATE: Socket will be the way to go.** We have chosen to create a custom communication protocol with sockets and keep alive available when the client application notices high demand on part of the user, this also allows us to have clearer view of which clients are connected and maintains a open channel with the client application, which might be useful for feature upgrades in the future.