From this answer about latencies, we have some numbers (yes, caveat caveat) for latencies when coding (slightly edited):   * L1 cache reference 0.5 ns   * Branch mispredict 5 ns   * L2 cache reference 7 ns   * Main memory reference 100 ns   * Send 2K bytes over 1 Gbps network 20,000 ns   * Read 1 MB sequentially from memory 250,000 ns   * Disk seek 10,000,000 ns   * Read 1 MB sequentially from network 10,000,000 ns   * Read 1 MB sequentially from disk 30,000,000 ns   * Send packet CA->Netherlands->CA 150,000,000 ns Do you have a metaphor that puts this in human terms? For example, a human might find a value in a table on their desk (L1) in 15s, so the equivalent time to go find the same thing if the page is still in the book on the shelf (L2) would be 14x longer, or 3.5 minutes. I know this seems like an ELI5, but I think most programmers (including me) lack a real grasp of how the orders of magnitude vary, making it hard to reason about whether to memoize some function or pre-cache some data. I'll expand the example in an answer. If you have a handle on CPU pipelining and cycles instead, that would be a great boon for understanding CPUs. Also mutex performance, SIMD paralellism, multi-threading, multi-core...