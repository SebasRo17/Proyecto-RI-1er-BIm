I think I understand the theoretical benefits of automated testing, especially unit testing. However, I'm not sure what the optimal amount of testing is when the project is a non-critical, rapidly developed (and deployed) web app with a reasonably large user base. My current reasoning goes like this:   * Most of the core functionality of our app can be manually tested in a matter of seconds. (The site is a type of search engine, so basically - run search, get results, it works.)   * Of course, there are many edge cases and options and such that could be tested, as well as unit testing of underlying logic, but if anything significant breaks and isn't caught before it's made live, we literally tend to get email from users within an hour. (Within minutes if it's a major problem, but very rarely does one of those get out.)   * So the consequence of any significant problem going live is that a few people are inconvenienced for a few minutes. Maybe some percentage of new visitors we get in that period would be lost, although the numbers should be very low since the time period is so short.   * Lower impact bugs might take longer for someone to email about - days, or even weeks depending on the severity - but they will also have less of an impact on users, so from a business perspective likely don't "matter" as much. Now, I'm not saying that excellent test coverage is useless. Of course it would be nice to catch every issue before it goes live. But in the cost/benefit tradeoff between working on tests and responding quickly to user feedback / adding and refining features, the latter seems to win. The one exception I've found is in code that doesn't directly affect the user. For example, ad serving code, or data collection. These don't directly impact the user's value from the site, so people are unlikely to let us know when they break. They will also be less noticeable in our feature testing. So I can see a strong case for well-defined unit tests there. (And from there, there ironically seems to be a spectrum where good test coverage becomes less important as features become more important, to the point where code that is critical to the site's operation doesn't really need to be tested (in a well defined manner) at all.) So my question is, am I way off base here? Is widespread test coverage and/or test-driven development optimal even in a non-critical, continuous-release environment? And if so, where am I failing in my reasoning? Or, assuming we fix bugs immediately when reported, is it reasonable to focus on rapid, nimble development, knowing that our users will let us know if we break something? (With the exception that we will have formal tests for functionality that is not directly exposed to the user.)