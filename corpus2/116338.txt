When Java was developed, it's designers chose to discard an unusual amount of the conventional wisdom established in the UNIX and C oriented toolchains. For one (and in my view) the most major example, they chose to discard the practice of header files and Makefiles and tried to build a monolithic, dependency-aware compiler instead. Later on, `ant` was developed to partially remedy the (obvious?) deficiencies of this process, and effectively re- implemented the makefile logic. It's not like I resent old-style make and `#include` going overboard. However, their usage was reinvented in the Java world with `ant` and splitting each class into an Interface and an Implementation class, which is pretty close to the header file/source file pattern. I'm fairly sure the people behind Java knew of the existing tools and patterns and their purpose, so the decision to first do away with them and then reimplementing everything from scratch without even a hint of it in the language design is what astonishes me. Less costly ways would have been available (like parsing the source file and auto- generating a header file, which would have been fairly easy after pruning the C syntax a bit), but weren't taken, and I would love to know about the reason. Similar forks were performed in most other places, at considerable development costs and the considerable cost of a splitted community. Nowadays, Java-world and C++-world developers often have a hard time communicating and collaborating due to a split that seems disproportionate to the actual differences between the C/C++ language and the Java language. While I can see the basic motivation behind the process (everyone loathes Makefile syntax and changing header files manually), I still can't grok the magnitude of the change. Are there historical references about the motivation and reasons of this process? Are there any design documents or interviews with the people who drove this fork?