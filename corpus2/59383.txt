I have a normal distribution with mean m and standard deviation d. How can I calculate what will be the deviation if I add N such distributions? As I understand, mean will be m*N. Will standard deviation be d*N? P.S. It looks like I should be using standard variances vs standard deviation. Sum of Gauss(Mean1, Standard Variance1) + Gauss(Mean2, Standard Variance2) = Gauss (Mean1 + Mean2, Standard Variance1 + Standard Variance 2). This way standard variance for N will be v*N, which mean that d=sqrt(N)*d