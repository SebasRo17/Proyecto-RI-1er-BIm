I am currently creating an API and implemented a few basic REST actions such as get, update, search etc. Another program that collects data needs to synchronize these to this API. My current thought is: What would be good practice to synchronize this data? A few informations: The amount of data that needs to be synchronized is different between applications. Some may transfer 100 records a minute, others 10k records a minute. There will be a transfer limit per hour to keep the server load balanced. Additionally, requests are signed with a token to check authenticity. Here's a little pro/con list I've made just now. I'm neither sure if these are real pros/cons there nor am I sure if I should weight some of them. **Option One**   Loop through this data and create an API call for every record. Pros:   * integrates better with the current REST API   * failed synchronisation entries can be queued for a resend Cons:   * a bigger overhead because of multiple calls **Option Two**   Build a package of all those data and send over that package in one call. Pros:   * less overhead   * just one query Cons:   * transfer error may drop all data   * separate implementation to allow