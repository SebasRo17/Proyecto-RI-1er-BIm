When outputting data in a large file, I try to make it a habit to incrementally output data in chunks, to prevent reading the entire large file into memory. As an example to show what I mean:               $fp = fopen('path/to/file.txt', 'r');     while (!feof($fp)) {         echo fread($fp, 8192);     }     fclose($fp);      Would this same practice be possible when fetching values from a database result? If so, how? As far as I'm aware, the ability to fetch unbuffered results from MySQL only applies to a row-by-row basis - and not per column. A basic example is shown below (I am aware that the mysql extension is deprecated - this is merely for illustrative purposes):               $q = mysql_incremental('SELECT `filedata` FROM `large_files` WHERE `id` = 1');     while (!mysql_incremental_eof($q, 'filedata')) {         echo mysql_fetch_incremental($q, 'filedata');     }     mysql_free_result($q);      A possible solution that I thought of is to combine multiple queries for portions of the file, making use of the `SUBSTRING()` function available in MySQL. I would imagine that although this might be more memory efficient, I'm wondering what the effect would be on network usage as well as disk activity?