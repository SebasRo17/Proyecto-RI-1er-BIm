In numerics, it is very important to be able to identify unstable schemes and to improve their stability. How to identify unstable floating point computations? I am working on a very complex simulation where many numerical schemes work together and I am looking for a methodic to identify its weak parts. I am working on a physical model involving differential equations. A bird's eye view of the overall process is:   1. (Preliminary step) Gather physical observations _P_.   2. Determine initial parameters of the simulation. This uses an optimisation algorithm, where we walk in a parameter space and look for parameters _C_ such that some error function _E(F(C), P)_ is minimised, where _F_ is some derived quantity of the parameters.   3. Plug _C_ in the simulation engine. This is an Euler scheme of the EDP, so that at each time step, we compute the terms driving the dynamic (each of them is a complex function, potentially subject to instability) and feed the Euler scheme with these dynamic terms to compute the next state. This goes on for thousands time points.   4. At the end of the simulation, we compute some function _Proof(S)_ of the final state _S_ and compare to some quantities _Require(P)_ deduced from the observed quantities. This is not a formal proof of the result, more a plausibility check. Also, I see a tower of complex operations (computation of dynamic terms, within the Euler scheme, within the _Proof_ ). And would like to recognise “bad parts” and fix them. I speculate that using a software implementation of floating point numbers with reduced precision would magnify the unstability of numerical schemes, thus easing the comparison between different implementations. Is this a common technique to investigate this question? Is it possible to use a virtual machine, as Bochs, to achieve this without altering the program? To deal appropriately with the stability question, it is sometimes acceptable to target the typical input of the numerical procedure, so that it can be tuned to do well on that input and maybe less well on other valid, yet unlikely, input. Given a sample of typical inputs, it is possible to _snoop_ some intermediate results and prepare a statistical profile for them. Again, is this a common technique to study stability issues? Is a virtual machine useful for this?