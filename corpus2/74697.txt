Unit testing consists of executable code which exercise a certain functionality and then assert some conditions. The results are either Pass or Fail.   * The objective is always to have all tests passed. Software performance profiling usually requires help from some automated test/profiling tool, but the results seem to always require human interpretation, because:   * There are no clear pass/fail objectives.   * There are many facets (dimensions) in software performance, and one needs to consider the overall performance profile (the full picture; no pun intended) in order to interpret the result and make a decision.   * There are many pitfalls in software profiling, such as results skewed by profiling overhead, unintended interactions between modules or operating system, test cases not representative of needs of customers. Now that unit testing has been heralded as a major breakthrough in promoting the credibility of software engineering, is it possible to find the next silver bullet for the software performance issues? **Clarification** on "automated to a high degree":   * What I mean is that unit tests that were written long time ago (and updated as the software is developed) will remain useful indefinitely. During the better part of the software's production lifetime, there is little to no need of modifying the unit tests unless there are big changes to functionality.   * On the other hand, performance testing seems to require adjustments and re-running continuously, even well into the production stage. **Related:**   * How is performance testing integrated into your product lifecycle?