I am trying to better understand what would be required for a compiler to be able to make intelligent choices regarding concurrency on behalf of the programmer. I realize that there are many difficult aspects of this problem for instance:   * Ensuring that there are no race conditions   * Ensuring that the code to be run concurrently won't have side effects that impact the semantic meaning of the code   * Deciding whether the overhead from spinning up threads is worthwhile given the degree of parallelism available in the code My understanding is that the two major intermediate representations used in modern compilers are static single assignment for procedural and object oriented languages and continuations passing style for functional languages. Reasoning about any of the problems listed above seems difficult using these intermediate forms. Even languages that should in theory have the best chance at automatic parallelization (pure functional languages like Haskell with a guarantee of no side effects) have made limited progress on this front. So my question is really what intermediate representations have been used to try and tackle this problem? Are there other representations that have been used in academic research that I am not aware of that are better suited to this task? Is this problem one that fundamentally has to be resolved by the compiler front end by manipulating the abstract syntax tree before compilation reaches an intermediate representation?