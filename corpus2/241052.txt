There are many questions about the performance of commands or scripts on the _U+L_ SE asset. As time is of the essence, this is often evaluated using the `bash` time _reserved word_ or the external `time` command and a subset of the target data, under average or no load. Yet there are scenarios which are either about a large number of files or just operations on very big files. In some instances a careful examination of what happens in the shell, intricate knowledge of command behavior, even hardware considerations, provide insight into efficiency. But benchmarking remains no simple affair. In one instance recently a community member made a comment about the _time complexity_ of a command, implying that since there was no _sorting operation_ \- and not merely no `sort` command - it would ultimately scale better with more data. The solution involved `awk`, whereas another solution involved for instance a combination of the `grep`, `sort` and `head` commands.   * Is this just an instance of "the simpler the better" - what are main implications of _time complexity_ when designing shell scripts which operate on huge data sets(number or size)?    * Beyond common sense and _sorting_ (an operation for which there is a seemingly high level of intuition about performance costs, even for a non expert), is there a well known _common time complexity_ example which would showcase how the concepts apply in practice to shell scripting?