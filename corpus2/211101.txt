I've been working on some MSTest automated test infrastructure, that is testing a tool that merges data sets into SQL Server database tables. The basic structure of the test is to:   1. Define the incoming dataset using anonymous types   2. Apply the data using the reconcile tool   3. Read records from the output tables   4. Compare result rows to input data, column by column Example input data:               public class InputData : List<dynamic> {} // Inspired by Massive etc          InputData input = new InputData()     {         new { ExternalID = 1, PropertyName = "hello", Agent = "test" },         new { ExternalID = (Int64)2, PropertyName = "fred", Agent = "test" } // Fixes the problem with a cast     };      The issue I'm dealing with, is that the type inferred on my anonymous objects will be an Int32, but the corresponding column in my target table is a bigint, and hence the record will have an Int64 value. As a result, when I use Assert.AreEqual across each column, it fails on any int fields:               Assert.AreEqual failed. Expected:<1 (System.Int32)>. Actual:<1 (System.Int64)>      You can see I have cast the int on my second anonymous object, this can be used to fix the issue. The primary aim of these tests is to make the sample data as slim and easy to read/write as possible, and I'd prefer to avoid the visual noise of all those casts. I'm thinking about the best way to deal with the assertions. It seems like I should use dedicated assertions based on type. I guess the real question is, how aggressive should I be in converting between types automatically?