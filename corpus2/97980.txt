I'm playing around with the .NET BigInteger and basically I'm wondering what number _\--an estimated answer would be fine--_ is the point of deviation of the curve of (the graph of (increase of time required for operations) vs (value of BigInteger))? or are they designed with no such deviation such that if we plot the increase of time required for operations vs value of BigInteger from 1 to infinity, we will have a smooth curve all the way? for example, assuming arrays are designed with capability of handling 50 items . this means that if i have 1 item, operations are f(1) time. and when i have 2 items, operations are f(2) time. if i have 50 items, operations are f(50) time. but since it is designed for handling 50 items only, the operations done when we have 51 items will be g(51) where g(51) > f(51). > If implemented properly the complexity of BigInteger arithmetic should be a > smooth curve. For example the time complexity of multiplication should be > O(NM) where N is the number of digits in the first multiplicand, and M is > the number of digits in the second multiplicand. Of course there are > practical limits in that you could pick N and M so large that the numbers > wouldn't fit in your machine. Are there any / does anyone know of any documents claiming that it is implemented as such?