I encounter strange behaviour of _Mathematica_ when evaluating multi- dimensional arrays of double-precision data. _Mathematica_ can express large arrays of machine-precision numbers using a specialized internal data-format, which optimizes memory usage and speeds up operations. However, _Mathematica_ does not always recognize when it can use such optimized arrays. Below are three cases that would ideally yield the same optimized data structure upon evaluation:               arr1 = Table[1.23, {1}, {100000}];     ByteCount[arr1]     (* 800168 *)          ncols = 1000;     arr2 = Table[1.23, {1}, {ncols}];     ByteCount[arr2]     (* 3200088 *)          ncols = 1000;     arr3 = Table[1.23, {1}, Evaluate[{ncols}]];     ByteCount[arr3]     (* 800168 *)      For some reason, the second case gives sub-optimal behaviour. My question is this: If I end up with a case where _Mathematica_ does not use its optimized numeric array data structure (such as `arr2`, above), how can I explicitly force a conversion?