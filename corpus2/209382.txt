In programming languages, for represent numbers you have (mainly) two types: `Int(s)` and `floats`. The us of `Int` is very easy. However floats cause a lot of unexpected surprises: http://anh.cs.luc.edu/python/hands-on/3.1/handsonHtml/float.html I wonder why is not used a decimal-based type make more sense as the default for literals and normal math? Where normal math is more precise?