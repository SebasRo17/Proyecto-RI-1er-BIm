I'm reading "Rock Breaks Scissors", which describes two "outguessing machines" built at Bell Labs that try to exploit human non-randomness in the game of matching pennies. There was an outguessing machine built by the famous Claude Shannon, and one built by Dave Hagelbarger. The machines worked like this (quoted from the book): > Suppose you win twice in a row with the same choice. What would you pick > next? You could stick with the choice that's been winning - or you could > switch, perhaps on the grounds that three times in a row wouldn't be > "random." > > Each time a particular situation occurred, [Shannon's] machine archived what > the player had decided. Every decision was encoded as a 1 or 0 and slotted > into one of the machine's 16 precious bits of memory. For each of the eight > given situations, Shannon's machine cataloged the last two decisions only. > That filled the machine's 16 bits. > > When the machine needed to predict, it looked at what the player had done > the last two times that the same situation had occurred. Whenever the > player's response had been identical both times, the machine predicted that > the player would do the same thing once again. Otherwise, it guessed > randomly from its [random number generator]. > > The main difference between Shannon's machine and Hagelbarger's was that > Shannon's was simpler. Hagelbarger's machine kept track of a percentage of > outcomes for each of the eight situations. The higher the percentage, the > more likely Hagelbarger's machine was to predict a repeat of the past. This > may sound more reasonable and nuanced than Shannon's all-or-nothing > approach, but in practice, Shannon's device was the better predictor. Shannon's machine beat humans at a higher rate than Hagelbarger's, and also beat Hagelbarger's machine when they played each other. I don't understand why Shannon's simpler approach was significantly better than Hagelbarger's percentage-based approach.