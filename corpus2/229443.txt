Well I'm no expert, but as a student, I'm curious about languages and their design patterns / goals. **I'd like to know, whether there are any points I miss in the following examples, and why techniques like this are not widely used in popular languages. Why is it a better idea in a real-life program to explicitly define all unit tests, instead of using some declarative code. why don't languages use some declarations as tests and implementation as well in general? Why split these thing into two parts?** Suppose you have the following unit tests:               assertRaises(avg([]), ValueError)     assertEquals(avg([1], 1)     assertEquals(avg([1, -2]), -.5)     assertEquals(avg([1, 2]), .5)      If you take this pseudocode,               def avg(items):         if len(items) == 0:             raise ValueError         else if len(items) == 1:             return items[0]         else:             return sum(items)/len(items)      most parts of `avg` seem to do nothing, or very little more than just declaring the way to pass some unit tests. In other words, for me it feels a little like code duplication. Especially the first test seems to be superfluous in this example. I wonder if it would be a good design to get rid of some unit test this way:               def avg(items) {         len(items) == 0 => raises ValueError         len(items) == 1 => items[0]         len(items) > 1 => sum(items)/len(items)         (test) items == [1, -2] => $avg == -.5         (test) items == [1, 2] => $avg == .5         (test) avg(items) <= sum(items)     }      In this example, (test) indicates when a specific test case is not a useful definition (though maybe these could even be used for optimization in rare cases). Of course, this pseudocode is not well designed because it's less readable than the first implementation, but I think it's a more convenient way to declare unit tests. Actually the unit tests seem to be wrong here, since they do nothing but test the definition. And the last test is not trivial to perform. But it can be performed for example, when another test is run, which tests a code that has `avg` in it, so chances are, that it gives a useful set of test parameters. Also, for example, the last unit test has a secondary role too! If I have a code like this:               if avg(a) <= sum(a)      It's not necessary at all to evaluate the complex definition, since at runtime one of the test cases implies it directly. OK, in this case, it's not a real optimization (though an IDE could notify the user, that the expression will always be true, which is useful). However, I can imagine, that there are many complex examples, that can be optimized this way, without making the developer explicitly test for special cases at every single place where they are relevant. Maybe a better example for this optimization               def power(a, n) {         n == 0 => 1         n == 1 => a         n > 1 => a * power(a, n - 1)         (test) for any (x) power(a, n) % x == power(a % x, n) % x     }      It's not secret I think, that a clever compiler could use it in many cases, and a clever interpreter/just-in-time compiler could use these in much more cases. In order to understand tests and languages better, I'm curious whether my thoughts are useful, and if not, what am I missing, or if so, why popular languages don't implement such patterns.