Suppose I have an array of float values in the following format:               { 1.34, 1.15, 1.1, 1.2, 1.26, 1.10, 1.20, 1.17 }      Suppose they have been provided by user input (or some other mechanism), where the user takes "1.1" to actually mean "1.01" - meaning that it is separate from "1.10". Using a standard sorting algorithm (bubble sort, quick sort, or some framework specific sort) is going to result in an array which should look similar to the following:               { 1.1, 1.10, 1.15, 1.17, 1.2, 1.20, 1.26, 1.34 }      However, the required output array would be the following:               { 1.1, 1.2, 1.10, 1.15, 1.17, 1.20, 1.26, 1.34 }      I'm thinking that the way to do this would be to iterate through the array before sorting and:   * check whether the value of has N decimal places   * if so, elevate to M (N + 1) decimal places - add a leading 0? This would lead to two arrays, one containing values with N or M number decimal places (raw user input) and one containing all values with M number decimal places ("sanitised" user input). Which would mean that sorting the array containing values with M decimal places would provide the required result. Are there any other ways to do this? I'm looking for faster algorithms, or those with a lower overhead. I see this as being a common problem domain, and I'm going to guess that there are a multitude of ways to fix this. What are some of the other algorithms that can be used to attain the required result.