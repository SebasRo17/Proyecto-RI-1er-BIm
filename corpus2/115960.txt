I am reading the following in a book on algorithms (Cormen's to be specific): > That is, we are concerned with how the running time of an algorithm > increases with the size of the input in the limit, as the size of the input > increases without bound. I can not understand what is the meaning of the phrase: `with the size of the input in the limit`.   What is the limit refering about? **UPDATE:** Please note that at this point in the book (early fundamentals of chapter 2) there has been no formal definition or mention of Big O or other asymptotic notation except a vague reference on Theta notation Any help?