_There are plenty of questions around regarding random number generation, but I couldn't find anything that exactly matched my question. Apologies if I missed one._ Most random number generators in software tend to have flat distribution - in the given range, any number is as likely to come next as any other (more or less). This is obviously the intention, because anything other than a flat distribution isn't entirely random because some numbers are more likely than others. However, there are times when a flat distribution is not desirable. For example, when building a game, it is often necessary to model real-world scenarios, many of which have a normal (bell-curve) distribution. If a "good" PRNG (such as an existing RNG from a popular programming language's main library) is fed into a simple equation, such as the equation for a bell curve, or a sine wave, or some other distribution, how far can we assume that the resulting PRNG is also "good"? Most of the tests for PRNGs test whether the system is purely random, so they won't work on this system. Is it just a matter of counting outputs over a long time and seeing how well they fit the desired distribution, or is there a simpler (eg. analytical or algebraic) way to check this? Any help would be appreciated.