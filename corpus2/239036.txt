I was watching this video on the maximum and minimum values of signed integers. Take an example of a positive signed value - 0000 0001 The first bit denotes that the number is positive and the last 7 bits are the number itself. So it is easily interpreted as +1. Now take an example of a negative signed value - 1000 0000 which comes out to be -8. Okay, the computer can understand that it is a negative value because of the first bit but how the hell does it understand that 000 0000 means -8? In general, how are negative signed values stored/interpreted in a computer?