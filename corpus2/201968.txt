_When writing an interpreter, how should the type inference algorithm chance the parsed AST? Should it?_ Or parsing and inference are done necessarily simultaneously? I have implemented a strongly typed functional language interpreter, in which each function can be declared only one time (i.e., with no signature variants). As soon as the non-typed implementation parser was concluded and working, I have written the type inference algorithm, feeded by the parser's resulting AST. Hindley- _Damas_ -Milner is working like a charm, but the program use it and if the type checking passes, it just forgets about the matter and pass the AST to the interpreter. To the moment, the "target language" (i.e., the one in which the interpreter is written) have satisfied the interpreter type needs, but **I am wondering if this approach is usual: to discard all the type inference fruits after an "ok" result.** Also, would writing a compiler instead of an interpreter make any difference is this sense?