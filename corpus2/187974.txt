Imagine a hypothetical programming environment that is largely like Java or .NET, i.e. object-oriented, garbage-collected, etc., but with one small change: Every time you call a method on an object, a lock is obtained so that no two threads can execute methods on the same object at the same time. The lock is relinquished when the method returns. The lock is also temporarily relinquished when you call Thread.Join (this is just to prevent certain kinds of deadlock situations). How severe would the performance impact of that really be? How badly would this interfere with concurrency? Are there any algorithms/programs that it would be impossible to parallelise in this kind of environment? In my previously posted question, a link was given to this page about the GIL in CPython which claims severe performance and concurrency problems with this. However, it also states that this environment uses reference counting and the increment/decrement of that reference count is an operation that obtains such a lock. Let's not assume that; .NET's GC is mark-and-sweep, not reference- counted.