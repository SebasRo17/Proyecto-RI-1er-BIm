I'm currently working on a project that requires a complicated model structure and I'm struggling with picking the right architecture. First of all, there are several interdependent models. Change in one model can trigger changes in several other models. For example, user inputs a query, first model checks the database for type of the query and the second model requests its data depending on that query type. Secondly, each model can have several data sources. Data sources are used conditionally, dependent on system configuration. The app needs to function seamlessly, â€” for example, if one data source doesn't return required information, model must try and use another one. An important thing also is that the codebase will eventually be expanded with other models. So models, data sources etc. must be very modular. Currently there are several ways I can organise the model layer:   1. 'Dumb' models re-created by factory objects when input data (or a related model) changes. Model dependencies are resolved in an operation object, encapsulating the whole update routine. Operation object is configured and run by a domain model. I don't know where to inject a data source though.   2. 'Smart' models with update logic encapsulated in them. Each model queries a sort of a mapper/data-source-manager, which works as a proxy for different data sources. Models report their updates to a domain model. Dependencies are resolved by a model-controller or dependency manager object. Which architecture is more appropriate here? I'm leaning towards the first one as it is (to my mind) better to work with immutable models, updating the whole tree on change of input. This approach gives several benefits: for example it is easier to serialize as the presence of model equals to a finite state. But I'm really unsure about data sources...