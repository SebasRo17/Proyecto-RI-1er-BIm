In the wake of the heartbleed bug, OpenSSL has been rightly critizised for using its own freelist. With plain-old `malloc`, the bug would have almost certainly been found long ago. However, some kind of malloc wrapping is actually very common. For example when I have a big object that owns many small, fixed sized objects, I often allocate them from an array.               /* BigObj is allocated with `malloc`, but LittleObj using `alloc_little`. */     typedef struct {              /* bigObj stuff here */              int nlittle;         LittleObj littles[MAX_LITTLE];     } BigObj;           LittleObj *alloc_little(BigObj *big)     {         if(big->nlittle == MAX_LITTLE)             return NULL;         return = big->littles + big->nlittle++;     }      `alloc_little` is faster than `malloc()`, but the main reason is simplicity. There is no `free_little` to call, because the `LittleObjs` just remain valid until the `BigObj` is destroyed. So is this an anti-pattern? If so, can I mitigate the risks, or should I just abandon it? Similar questions go for any kind of memory pool, such as GNU Obstacks and the `talloc` used in Samba.