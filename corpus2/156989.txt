It is common to use idioms such as:               x/60.0      to force a floating-point division when `x` is an integer in languages which do not have distinct operators for integer and decimal division. Is this an accepted idiom or is it preferrable to cast `x` to a floating-point type? I believe it is a concise and clear idiom- it might be show intent less clearly than an explicit cast, but it is more concise and easier to read. I think that it's less hack-ish than `x + ""` to convert to string or `x+0.0` to convert to floating point (as there's no unnecessary "NOOP"). Thoughts? √Ålex