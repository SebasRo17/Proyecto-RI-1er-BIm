From what I've read, genetic algorithms are usually (always?) applied to chromosomes of bits. So if a problem involves maximizing a function that takes integer values, the integers are first encoded as bits. Is this mapping necessary? It seems that you could take chromosomes of integers and apply crossover and mutation directly. So, if I have a function that takes 35 integer inputs, I can just apply the genetic operators to those integers, rather than on the 35xB bits (where B is the number of bits needed to encode an integer). Is there a reason this isn't done? Perhaps the algorithm would suffer because the problem is defined more coarsely (that is, a problem can be defined with shorter chromosomes if we're not using bits), but I'm curious if anyone has a better answer.