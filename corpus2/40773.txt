I would like to use `TensorReduce` by assuming that certain patterns of functions are tensors. From documentation of `TensorReduce`: > If `TensorDimensions[ten]` does not return a list of dimensions, then the > expression ten is returned unchanged. I would have inferred from above that if I modify `TensorDimensions[ten]`, `TensorReduce` should work. Thus I did               Unprotect[TensorDimensions];     TensorDimensions[f_[g__]] := d & /@ {g};     Protect[TensorDimensions];          Assuming[ t ∈ Arrays[{d, d}, Antisymmetric[All]] ,      TensorReduce @ TensorContract[ t\[TensorProduct]f[DN, DN], {1, 4}]]      However, this doesn't work. i.e. `TensorReduce` does nothing, and the result is               TensorContract[ t\[TensorProduct]f[DN, DN], {{1, 4}}]      To compare, having defined               Assuming[ t ∈ Arrays[{d, d}, Antisymmetric[All]] &&                f[DN, DN] ∈ Arrays[{d, d}],      TensorReduce @ TensorContract[ t\[TensorProduct]f[DN, DN], {1, 4}]]      > >     - TensorContract[ t\[TensorProduct]f[DN, DN], {{2, 4}}] >   the result is indeed simplified as desired: I'd like to understand what are the assumptions that `TensorReduce` really uses. Is there a way that I can work with `TensorReduce` as above, with pattern like declaration of tensors? PS: Currently I generate a list of assumptions of `f_[g__]` using `Cases`, and put those assumptions together in `Assuming`. This makes the code slow and ugly.