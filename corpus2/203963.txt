How do interpreted languages typically go about representing floating-point numbers in their bytecode? For example, suppose I've got a Java program with the line `double a = 3.0;` What does `3.0` look like in the program's bytecode? I see two possibilities: 1) Writing the bit-level representation of the in-memory number into the bytecode, and then just reading it back into memory when we need it. There are probably problems with portability here (the size of `double` on different machines can vary, so how do we store a number in bytecode in a portable way?). 2) Write a canonical representation of the number into the bytecode which can be decoded when program execution time comes. The literal string `3.0` might serve that purpose here, but then we're wasting execution time parsing a floating-point number during runtime. So my question is, which strategy is most common/efficient/best? You can feel free to cite examples from different interpreted languages that compile down to bytecode (Python, Lua, etc.), or merely point me in the direction of some resources of some relevant articles if that's necessary (I've been Googling quite a bit, but few virtual machines are well-documented).