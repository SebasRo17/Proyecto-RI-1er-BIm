Do commercial statistical calculators like Minitab or SAS (and open source like R) perform statistical analysis with very high precision? Or do they just use floating point (or double) precision? I want to write a statistical calculator that professional statisticians can use for any purpose. I've written a program that currently performs or calculates standardized z-score, hypothesis tests, confidence intervals, and descriptive statistics. Right now 90% of the calculations use an arbitrary precision library (C++) or Java's BigDecimal class. I'm almost certain now that this is a mistake because even if the statistic is for monetary values, a statistic is usually rounded and the individual values that form the composition of the statistic are also rounded by four decimal places (or temporary shifted to appear as integers). However, I suspect that there is a flaw in my logic and I can't really say why precision is important. I suspect that with double precision in a standard library, even the calculation of mean could be wildly inaccurate. Perhaps I want to know the mean of a set of very small number that has too many decimal places to be supported by floating point precision.