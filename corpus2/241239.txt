So we use hexadecimal which has the advantage of going up to 15 in single digits A-F, but why is it an issue that it takes 2 digits to represent the number 10 in decimal? I was reading up about hexadecimal and I came across these 2 lines: > Base 16 suggests the digits 0 to 15, but the problem we face is that it > requires 2 digits to represent 10 to 15. Hexadecimal solves this problem by > using the letters A to F. My question is, why should we care how many digits it takes to represent a number? - Is it slower for the computer to deal with a number with 2 digits compared to 1?