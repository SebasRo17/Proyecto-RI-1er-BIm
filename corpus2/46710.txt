I have written a SOR method (SOR is this method) code using C-style procedural loops. However, I think there might be (much) better ways to achieve the same end in Mma avoiding these loops. So my question is: Is that feasible to code SOR method using functions like `Nest`, `Fold` that could be substantially more efficient? For example, the following is a simple set of multiline equations with tridiagonal matrix.(my original equations will have an larger coefficient matrix, like 100*100 tridiagonal sparse array.) ![](http://i.stack.imgur.com/h1VQV.png)   1. Let `listnew` be the list that stores the unknown $x_1,x_2,x_3,x_4,x_5$ after each time of iteration.   2. And `listold` correspondingly stores the unknowns before each time of iteration.   3. `b` stores right-hand side of the equation above.   4. `k` indicates the times of iteration.   5. `i` is the index of $x_i$.   6. `ϵ` is the pre-defined maximum error.   7. `error` indicates the maximum difference between `listnew` and `listold` after each iteration.   8. `ω` is the so-called relaxation factor. Then, my (dumb) code using `For` loop would be:               ϵ = 0.1;     ω = 1.5;     b = {1, 1, 1, 1, 1};     listnew = listold = Table[0, {i, 5}];     error = ϵ + 1;     For[k = 0, error > ϵ, k++,         listnew[[1]] = listold[[1]] - ω (b[[1]] + 4 listold[[1]] - listold[[2]])/4;         For[i = 2, i < 5, i++,              listnew[[i]] = listold[[i]] -                             ω (b[[i]] + 4 listold[[i]] - listnew[[i - 1]] - listold[[i + 1]])/4];         listnew[[5]] = listold[[5]] - ω (b[[5]] + 4 listold[[5]] - listnew[[4]])/4;         error = Max[Abs[listnew - listold]]; listold = listnew;]      I'm sorry if it looks so dumb...