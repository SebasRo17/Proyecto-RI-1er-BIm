apologise advance length post want paint accurate picture problem app facing pose question trying address self design pain leading application crashing due memory error abridged description problem domain follows application take dataset consists numerous text file containing related individual text file within dataset usually contains approx num header contain metadata contains also contains large tab delimited section containing related one text file contained within dataset number column per file num num column original application written allow user load dataset map certain column file basically indicating key information file show related well identify expected column name done validation process take place enforce various rule ensure relationship file valid done imported sql server database database design eav entity attribute model used cater column per file know eav detractor case feel reasonable choice given disparate number column submitted dataset memory problem given fact combined size text file num meg effort reduce database transaction time decided read file memory perform following perform validation whilst memory relate using model start db transaction write key column row row noting id written row table database utilise identity column id newly written row applied related related updated key information relates record written using sqlbulkcopy due eav model essentially x column row write x num row often ten thousand written without take several minute large datasets commit transaction problem come fact receiving individual file containing num meg dataset receive number file started seen datasets around num meg coming expect going get bigger file size even read memory without app falling let alone validated imported anticipate modify large chunk allow validation occur parsing file exactly decided handle import transaction potential improvement wondered using guids relate rather relying identity field would allow related prior writing database would certainly increase storage required though especially eav design would think reasonable thing try simply persist identity field natural key trusted unique across submitter use staging table get database performing transaction copy staging area actual destination table question system like import large quantity go keeping transaction small kept small possible current design still active several minute write hundred thousand record one transaction better solution tab delimited section read datatable viewed grid need full functionality datatable suspect overkill anyway turn various feature datatables make lightweight obvious thing would situation minimise memory footprint application described thanks kind attention