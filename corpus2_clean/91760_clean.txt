going developing functionality crawl various public web site process aggregate nothing sinister like looking e mail address fact something might actually drive additional traffic site digress honouring robot txt rule guideline written unwritten ought following order avoid appearing malicious potentially banned b cause problem site owner webmaster example think may may matter number parallel request time request time entire crawl avoiding potentially destructive link want spider doom know even practical really spit balling though tried tested wisdom broadly applicable anybody intends write utilize spider