question two related part first dataset size second data wrangling part built naive bayesian classifier mathematica us kernelmixturedistribution train large mm row dataset field used two separate class trained unfortunately try load mm row mathematica get java heap space error end world load column individually compute distribution save load distribution separately without training data later problem come prediction part process mb k row csv file load traditional environment either read row time slow prediction write separate file read block row time see way mathematica import seems import whole file run memory gig people working large datasets fit memory part dealing data mostly column shifted bit sometimes extra column added mathematica seem record structure type list matrix people dealing taken loading data sql database even suboptimal classifier make sure select field order offset list edit look like mathematica support sort cursor called result set look slow k record one time computer time cheap person time going attempt train classifier per normal use result set individually classify sample row work post answer question detail making one huge presumption mma java database link free memory row longer needed long case approach might stand chance working edit playing around found mathematica result set act standard select would load entire dataset jdbc mathematica memory approach run memory like standard import select would hit another approach relying database postgresql lifting using cursor approach work process data sequentially acceptable limitation first declared cursor predrows made sure would persist outside transaction hold part sqlexecute dbconn declare predrows cursor hold select mytable next fetch row cursor iterate k row one time request also ask row say time like fetch one row time sqlexecute dbconn fetch predrows fetch row ttime sqlexecute dbconn fetch predrows mean program imperative manner learn live postgres documentation cursor